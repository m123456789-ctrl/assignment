{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a711690-ddb8-424f-b7c5-38a6ba38e031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Q_1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79bbb6af-10c2-48aa-bbac-29431a13e1b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "countries = pd.read_csv(\"/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/2_silver/silver_finance/silver_countries.csv\")\n",
    "countries = countries[['name', 'alpha-2', 'alpha-3']]\n",
    "countries = countries.rename(columns={'name' : 'country-name'})\n",
    "\n",
    "ikea_stores = pd.read_csv(\"/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/2_silver/silver_stores/silver_stores.csv\")\n",
    "ikea_stores['alpha-2'] = ikea_stores['locale'].str[:2]\n",
    "ikea_stores['alpha-2'] = ikea_stores['alpha-2'].str.upper()\n",
    "ikea_stores = ikea_stores[['id', 'alpha-2']]\n",
    "ikea_stores = ikea_stores.drop_duplicates()\n",
    "\n",
    "pivot_table = pd.merge(ikea_stores, countries, on='alpha-2', how='inner')\n",
    "pivot_table = pivot_table.sort_values(by='id')\n",
    "\n",
    "counts = pivot_table.groupby(['country-name', 'alpha-2', 'alpha-3']).size().reset_index(name='sum_of_IKEA_stores')\n",
    "display(counts)\n",
    "counts.to_parquet('/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/3_gold/Q_1_pivot_table_sum_IKEA_stores.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98159c3c-0e37-4305-8f2e-180cd931056c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Q_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56998082-6173-4d96-8a03-808b86021874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "big_mac = pd.read_csv(\"/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/2_silver/silver_economist/silver_big-mac-data.csv\")\n",
    "#big_mac = big_mac.sort_values(by=['name', 'date'], ascending=[True, False])\n",
    "\n",
    "big_mac['date'] = pd.to_datetime(big_mac['date'])\n",
    "big_mac = big_mac[big_mac['date'] >= pd.to_datetime('today') - pd.DateOffset(years=5)]\n",
    "big_mac['date'] = big_mac['date'].dt.strftime('%Y-%m-%d')\n",
    "big_mac = big_mac.rename(columns={'iso_a3' : 'alpha-3'})\n",
    "\n",
    "merge_table = pd.merge(big_mac, counts, on='alpha-3', how='inner')\n",
    "#unique_countries_count = merge_table['name'].nunique()\n",
    "#print(unique_countries_count)\n",
    "\n",
    "\n",
    "# Find countries in df2 but not in df1\n",
    "#countries_in_df2_not_in_df1 = counts[~counts['alpha-3'].isin(merge_table['alpha-3'])]\n",
    "#display(countries_in_df2_not_in_df1)\n",
    "\n",
    "import numpy as np\n",
    "merge_table['price_USD'] = np.where(merge_table['dollar_ex'] != 0, merge_table['local_price'] / merge_table['dollar_ex'], np.nan).round(2)\n",
    "\n",
    "merge_table = merge_table[['name', 'local_price', 'price_USD','date']]\n",
    "merge_table = merge_table.drop_duplicates()\n",
    "\n",
    "table_5y_avg_price = merge_table.groupby('name')[['local_price', 'price_USD']].mean().round(2).reset_index()\n",
    "\n",
    "display(table_5y_avg_price)\n",
    "table_5y_avg_price.to_parquet('/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/3_gold/Q_2_table_5y_avg_price.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c59a6dd-d9cc-4fe7-a864-cc31600b85e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Q_3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3baba14d-c006-4f22-bc57-26c0832ee720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merge_table['date'] = pd.to_datetime(merge_table['date'])\n",
    "latest_price = merge_table.sort_values(by=['name', 'date'], ascending=[True, False])\n",
    "latest_price = latest_price.drop_duplicates(subset='name', keep='first')\n",
    "latest_price = latest_price[['name', 'local_price', 'price_USD']]\n",
    "display(latest_price)\n",
    "latest_price.to_parquet('/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/3_gold/Q_3_latest_price.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cdc6e9b-268e-4ff2-8799-70b9b631feca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Specify the file path to your Parquet file\n",
    "file_path = '/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/3_gold/latest_price.parquet'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8155c66-bc95-43a4-a7f5-7c4f381765bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Big-mac-silver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a0c1538-fff1-4ecd-b863-ef8a2bfe4102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "input_path = \"/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/1_bronze/src_economist\"\n",
    "output_path = \"/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/2_silver/silver_economist\"\n",
    "file_name = \"big-mac-data\"\n",
    "\n",
    "batch_paths = [\n",
    "    f\"{input_path}/batch_1/big-mac-source-data-v2_1.csv\",\n",
    "    f\"{input_path}/batch_2/big-mac-source-data-v2_2.csv\",\n",
    "    f\"{input_path}/batch_3/big-mac-source-data-v2_3.csv\",\n",
    "]\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"iso_a3\", StringType(), True),\n",
    "    StructField(\"currency_code\", StringType(), True),\n",
    "    StructField(\"local_price\", DoubleType(), True),\n",
    "    StructField(\"dollar_ex\", DoubleType(), True),\n",
    "    StructField(\"GDP_dollar\", DoubleType(), True),\n",
    "    StructField(\"GDP_local\", DoubleType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "\n",
    "silver_big_mac = spark.createDataFrame(data=[], schema=schema)\n",
    "\n",
    "for path in batch_paths:\n",
    "    #raw_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "    raw_data = pd.read_csv(path)\n",
    "\n",
    "    # Convert Pandas DataFrame to PySpark DataFrame\n",
    "    raw_data = spark.createDataFrame(raw_data)\n",
    "\n",
    "    # Apply transformations\n",
    "    cleaned_data = raw_data.dropDuplicates()\n",
    "    # Write to Silver layer\n",
    "    #cleaned_data.toPandas().to_csv(f\"{output_path}/silver_{file_name}.csv\")\n",
    "    #cleaned_data.write.mode(\"append\").option(\"header\", \"true\").csv(f\"{output_path}/silver_{file_name}.csv\")\n",
    "    silver_big_mac = silver_big_mac.union(cleaned_data)\n",
    "\n",
    "silver_big_mac.toPandas().to_csv(f\"{output_path}/silver_{file_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecb05075-766d-468e-9ef2-4b6e0f5e6ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Stores-bronze JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f93157d-0c28-4392-b852-7c624c3719ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Step 1: Fetch data from the web\n",
    "url = \"https://www.ikea.com/global/assets/informera/stores/stores-unfiltered-detailed.json\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Step 2: Parse the response into JSON\n",
    "    data = response.json()\n",
    "    \n",
    "    # Step 3: Save the JSON file in Databricks File System (DBFS)\n",
    "    json_path = \"/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/1_bronze/src_stores/bronze_stores.json\"  # Adjust the path as needed\n",
    "    \n",
    "    with open(json_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    \n",
    "    print(f\"JSON file saved successfully at {json_path}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f818a84-538a-455f-ad77-dd80e1da99c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Stores-silver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c2581c-9ab0-488c-8eb4-ba89d876ef1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Step 1: Load the JSON file into a pandas DataFrame\n",
    "input_path = \"/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/1_bronze/src_stores/bronze_stores.json\"\n",
    "output_path = \"/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/2_silver/silver_stores\"\n",
    "file_name = \"stores\"\n",
    "\n",
    "data = pd.read_json(input_path)\n",
    "\n",
    "# Flatten the locales data\n",
    "flattened_data = []\n",
    "for index, row in data.iterrows():\n",
    "    id, lat, lng, locales = row[\"id\"], row[\"lat\"], row[\"lng\"], row[\"locales\"]\n",
    "    for locale_key, locale_value in locales.items():\n",
    "        entry = {\n",
    "            \"id\": id,\n",
    "            \"lat\": lat,\n",
    "            \"lng\": lng,\n",
    "            \"locale\": locale_key,\n",
    "            \"displayName\": locale_value[\"displayName\"],\n",
    "            \"displayNameAlternate\": locale_value[\"displayNameAlternate\"],\n",
    "            \"street\": locale_value[\"address\"].get(\"street\"),\n",
    "            \"zipCode\": locale_value[\"address\"].get(\"zipCode\"),\n",
    "            \"city\": locale_value[\"address\"].get(\"city\"),\n",
    "            \"timezone\": locale_value[\"address\"].get(\"timezone\"),\n",
    "            \"stateProvinceCode\": locale_value[\"address\"].get(\"stateProvinceCode\"),\n",
    "            \"displayAddress\": locale_value[\"address\"].get(\"displayAddress\"),\n",
    "        }\n",
    "        flattened_data.append(entry)\n",
    "\n",
    "# Create a new DataFrame from the flattened data\n",
    "df_flattened = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Step 3: Drop duplicates\n",
    "silver_stores = df_flattened.drop_duplicates()\n",
    "\n",
    "# Step 4: Save as CSV\n",
    "silver_stores.to_csv(f\"{output_path}/silver_{file_name}.csv\", index=False)\n",
    "\n",
    "print(f\"CSV file saved successfully at {output_path}/silver_{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b7f4cf6-ead4-4f31-8643-d3007668eb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Display data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbbfdc0e-3736-4a16-9ddf-00703ab3a25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required library\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/2_silver/silver_stores/silver_stores.csv\")\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d2fa56-455b-4711-8204-c5d2c8cfa714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required library\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"/Workspace/Users/a845678@asb.dtcbtndsie.onmicrosoft.com/assignment/data_storage/2_silver/silver_finance/silver_countries.csv\")\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook_with_answers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
