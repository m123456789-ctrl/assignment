{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b6cc92-8af6-4071-a790-142629cc4a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d11b2d4-4010-4df4-bf96-0f6d122e3253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ec5977-020a-4896-89de-a833c11360cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    \"\"\"Load pipeline configuration.\"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def validate_schema(df, schema):\n",
    "    \"\"\"Validate the schema of a DataFrame.\"\"\"\n",
    "    for column, dtype in schema.items():\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"Missing column: {column}\")\n",
    "        if not df.schema[column].dataType.simpleString() == dtype:\n",
    "            raise TypeError(f\"Column {column} expected type {dtype}, got {df.schema[column].dataType.simpleString()}\")\n",
    "    print(\"Schema validation passed!\")\n",
    "\n",
    "\n",
    "def apply_transformations(df, transformations):\n",
    "    \"\"\"Apply transformations based on configuration.\"\"\"\n",
    "    for transform in transformations:\n",
    "        if transform['type'] == 'filter':\n",
    "            df = df.filter(col(transform['column']) == transform['value'])\n",
    "        elif transform['type'] == 'select':\n",
    "            df = df.select(*transform['columns'])\n",
    "        elif transform['type'] == 'dropDuplicates':\n",
    "            df = df.dropDuplicates()\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_as_parquet(df, output_path, table_name):\n",
    "    \"\"\"Save DataFrame to Parquet format.\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = f\"{output_path}{table_name}_data_{timestamp}.parquet\"\n",
    "    df.write.parquet(output_file, mode='overwrite')\n",
    "    print(f\"Data for table '{table_name}' saved to {output_file}\")\n",
    "\n",
    "def save_as_csv(df, output_path, file_name, mode=\"overwrite\", header=True):\n",
    "    #df.write.csv(path=f\"file://{output_path}/silver_{file_name}.csv\", mode=mode, header=header)\n",
    "    df.toPandas().to_csv(f\"{output_path}/silver_{file_name}.csv\")\n",
    "    print(f\"Countries data saved as CSV at {output_path}/silver_{file_name}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "def process_table(table_config, spark):\n",
    "    \"\"\"Process a single table based on its configuration.\"\"\"\n",
    "    input_path = table_config['input_path']\n",
    "    output_path = table_config['output_path']\n",
    "    transformations = table_config['transformations']\n",
    "    schema = table_config['schema']\n",
    "    table_name = table_config['name']\n",
    "\n",
    "    # Load input data\n",
    "    print(f\"Processing table: {table_name}\")\n",
    "    #df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "    df = pd.read_csv(input_path)\n",
    "    spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "    # Convert Pandas DataFrame to PySpark DataFrame\n",
    "    df = spark.createDataFrame(df)\n",
    "\n",
    "    # Validate schema\n",
    "    validate_schema(df, schema)\n",
    "\n",
    "    # Apply transformations\n",
    "    df = apply_transformations(df, transformations)\n",
    "\n",
    "    # Save the processed data\n",
    "    #save_as_parquet(df, output_path, table_name)\n",
    "\n",
    "    # Save the processed data as csv\n",
    "    save_as_csv(df, output_path, table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "libraries_and_functions.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
